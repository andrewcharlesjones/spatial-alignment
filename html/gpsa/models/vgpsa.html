<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gpsa.models.vgpsa API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gpsa.models.vgpsa</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
import numpy as np
import torch.nn as nn
from sklearn.cluster import KMeans
from gpsa import GPSA
from ..util.util import rbf_kernel
from collections.abc import Iterable

torch.autograd.set_detect_anomaly(True)

device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;


class VariationalGPSA(GPSA):
    def __init__(
        self,
        data_dict,
        m_X_per_view,
        m_G,
        data_init=True,
        minmax_init=False,
        grid_init=False,
        n_spatial_dims=2,
        n_noise_variance_params=2,
        kernel_func_warp=rbf_kernel,
        kernel_func_data=rbf_kernel,
        n_latent_gps=None,
        mean_function=&#34;identity_fixed&#34;,
        mean_penalty_param=0.0,
        fixed_warp_kernel_variances=None,
        fixed_warp_kernel_lengthscales=None,
        fixed_data_kernel_lengthscales=None,
        fixed_view_idx=None,
    ):
        super(VariationalGPSA, self).__init__(
            data_dict,
            data_init=True,
            n_spatial_dims=2,
            n_noise_variance_params=2,
            kernel_func_warp=kernel_func_warp,
            kernel_func_data=kernel_func_data,
            mean_penalty_param=mean_penalty_param,
            fixed_warp_kernel_variances=fixed_warp_kernel_variances,
            fixed_warp_kernel_lengthscales=fixed_warp_kernel_lengthscales,
            fixed_data_kernel_lengthscales=fixed_data_kernel_lengthscales,
        )

        self.m_X_per_view = m_X_per_view
        self.m_G = m_G
        self.n_latent_gps = n_latent_gps
        self.n_latent_outputs = {}
        for mod in self.modality_names:
            curr_n_latent_outputs = (
                self.n_latent_gps[mod]
                if self.n_latent_gps[mod] is not None
                else self.Ps[mod]
            )
            self.n_latent_outputs[mod] = curr_n_latent_outputs
        self.fixed_view_idx = fixed_view_idx

        if data_init:
            # Initialize inducing locations with a subset of the data
            Xtilde = torch.zeros([self.n_views, self.m_X_per_view, self.n_spatial_dims])
            for ii in range(self.n_views):
                curr_X_spatial_list = []
                for mod in self.modality_names:
                    curr_idx = self.view_idx[mod][ii]
                    curr_modality_and_view_spatial = data_dict[mod][&#34;spatial_coords&#34;][
                        curr_idx, :
                    ]
                    curr_X_spatial_list.append(curr_modality_and_view_spatial)
                curr_X_spatial = torch.cat(curr_X_spatial_list, dim=0)

                kmeans = KMeans(n_clusters=self.m_X_per_view)
                kmeans.fit(curr_X_spatial.detach().cpu().numpy())
                Xtilde[ii, :, :] = torch.tensor(kmeans.cluster_centers_)

            self.Xtilde = nn.Parameter(Xtilde.clone())
            # self.Xtilde = Xtilde.clone()

            rand_idx = np.random.choice(
                np.arange(curr_X_spatial.shape[0]),
                size=self.m_G,
                replace=False,
            )

            all_X_spatial = torch.cat(
                [data_dict[mod][&#34;spatial_coords&#34;] for mod in self.modality_names]
            )
            kmeans = KMeans(n_clusters=self.m_G)
            kmeans.fit(all_X_spatial.detach().cpu().numpy())
            self.Gtilde = nn.Parameter(torch.tensor(kmeans.cluster_centers_))

        elif grid_init:

            if self.n_spatial_dims == 2:
                xlow, ylow = (
                    data_dict[self.modality_names[0]][&#34;spatial_coords&#34;].numpy().min(0)
                )
                xhigh, yhigh = (
                    data_dict[self.modality_names[0]][&#34;spatial_coords&#34;].numpy().max(0)
                )
                xlimits = [xlow, xhigh]
                ylimits = [ylow, yhigh]
                numticks = np.ceil(np.sqrt(self.m_G)).astype(int)
                self.m_G = numticks**2
                self.m_X_per_view = numticks**2
                x1s = np.linspace(*xlimits, num=numticks)
                x2s = np.linspace(*ylimits, num=numticks)
                X1, X2 = np.meshgrid(x1s, x2s)
                Xtilde = np.vstack([X1.ravel(), X2.ravel()]).T
                Xtilde_torch = torch.zeros(
                    [self.n_views, Xtilde.shape[0], self.n_spatial_dims]
                )
                for vv in range(self.n_views):
                    Xtilde_torch[vv] = torch.tensor(Xtilde)

                # self.Xtilde = Xtilde_torch.clone()
                # self.Gtilde = torch.tensor(Xtilde).float()
                self.Xtilde = nn.Parameter(Xtilde_torch.clone())
                self.Gtilde = nn.Parameter(torch.tensor(Xtilde).float())

        else:
            # Random initialization of inducing locations
            self.Xtilde = nn.Parameter(
                torch.randn([self.n_views, self.m_X_per_view, self.n_spatial_dims])
            )
            self.Gtilde = nn.Parameter(torch.randn([self.m_G, self.n_spatial_dims]))

        ## Variational covariance parameters
        Omega_sqt_G_list = torch.zeros(
            [self.n_views * self.n_spatial_dims, self.m_X_per_view, self.m_X_per_view],
            device=device,
        )
        for ii in range(self.n_views):
            for jj in range(self.n_spatial_dims):
                Omega_sqt = 0.1 * torch.randn(
                    size=[self.m_X_per_view, self.m_X_per_view]
                )
                # import ipdb; ipdb.set_trace()
                # Omega_sqt_G_list[ii * self.n_views + jj, :, :] = Omega_sqt
                Omega_sqt_G_list[jj * self.n_views + ii, :, :] = Omega_sqt
        self.Omega_sqt_G_list = nn.Parameter(Omega_sqt_G_list)

        Omega_sqt_F_dict = torch.nn.ParameterDict()
        for mod in self.modality_names:
            num_outputs = self.Ps[mod]
            curr_Omega = torch.zeros([self.n_latent_outputs[mod], self.m_G, self.m_G])
            for jj in range(self.n_latent_outputs[mod]):
                Omega_sqt = 0.1 * torch.randn(size=[self.m_G, self.m_G])
                curr_Omega[jj, :, :] = Omega_sqt
            Omega_sqt_F_dict[mod] = nn.Parameter(curr_Omega)
        self.Omega_sqt_F_dict = Omega_sqt_F_dict

        ## Variational mean parameters
        self.delta_G_list = nn.Parameter(self.Xtilde.clone())
        delta_F_dict = torch.nn.ParameterDict()
        for mod in self.modality_names:
            num_outputs = self.Ps[mod]
            curr_delta = nn.Parameter(
                torch.randn(size=[self.m_G, self.n_latent_outputs[mod]], device=device)
            )
            delta_F_dict[mod] = curr_delta
        self.delta_F_dict = delta_F_dict

        ## LMC parameters
        self.W_dict = torch.nn.ParameterDict()
        for mod in self.modality_names:
            if self.n_latent_gps[mod] is not None:
                self.W_dict[mod] = nn.Parameter(
                    torch.randn([self.n_latent_gps[mod], self.Ps[mod]])
                )

    def compute_mean_and_var(
        self, Kff_diag, Kuf, Kuu_chol, mu_x, mu_z, delta, Omega_tril
    ):
        alpha_x = torch.cholesky_solve(Kuf, Kuu_chol)

        a_t_Kchol = torch.matmul(alpha_x.transpose(-1, -2), Kuu_chol)
        aKa = torch.sum(torch.square(a_t_Kchol), dim=-1)

        mu_tilde = mu_x.unsqueeze(0) + torch.matmul(
            alpha_x.transpose(-1, -2), delta - mu_z
        )

        if len(alpha_x.shape) == 2:
            a_t_Omega_tril = torch.matmul(
                alpha_x.transpose(-1, -2).unsqueeze(0), Omega_tril
            )
            aOmega_a = torch.sum(torch.square(a_t_Omega_tril), dim=-1)
            Sigma_tilde = Kff_diag - aKa + aOmega_a + self.diagonal_offset
        else:
            a_t_Omega_tril = torch.matmul(
                alpha_x.transpose(-1, -2).unsqueeze(1), Omega_tril.unsqueeze(0)
            )
            aOmega_a = torch.sum(torch.square(a_t_Omega_tril), dim=-1)
            Sigma_tilde = (
                Kff_diag.unsqueeze(1)
                - aKa.unsqueeze(1)
                + aOmega_a
                + self.diagonal_offset
            )

        return mu_tilde, Sigma_tilde + self.diagonal_offset

    def get_Omega_from_Omega_sqt(self, Omega_sqt):
        return torch.matmul(
            Omega_sqt,
            torch.transpose(Omega_sqt, -1, -2),
        ) + self.diagonal_offset * torch.eye(Omega_sqt.shape[-1], device=device)

    def forward(self, X_spatial, view_idx, Ns, S=1, prediction_mode=False, G_test=None):

        if prediction_mode:
            self.eval()

        self.noise_variance_pos = torch.exp(self.noise_variance) + self.diagonal_offset

        self.mu_z_G = (
            torch.zeros(
                [self.n_views, self.m_X_per_view, self.n_spatial_dims], device=device
            )
            * np.nan
        )
        for vv in range(self.n_views):
            self.mu_z_G[vv] = (
                torch.mm(self.Xtilde[vv], self.mean_slopes[vv])
                + self.mean_intercepts[vv]
            )
            if self.fixed_view_idx is not None and (
                vv in self.fixed_view_idx
                if isinstance(self.fixed_view_idx, Iterable)
                else self.fixed_view_idx == vv
            ):
                self.mu_z_G[vv] *= 100.0

        self.Kuu_chol_list = (
            torch.zeros(
                [self.n_views, self.m_X_per_view, self.m_X_per_view], device=device
            )
            * np.nan
        )
        G_samples = {}
        for mod in self.modality_names:
            G_samples[mod] = (
                torch.zeros([S, Ns[mod], self.n_spatial_dims], device=device) * np.nan
            )

        G_means = {}
        for mod in self.modality_names:
            G_means[mod] = (
                torch.zeros([Ns[mod], self.n_spatial_dims], device=device) * np.nan
            )

        curr_Omega_G = self.get_Omega_from_Omega_sqt(self.Omega_sqt_G_list)

        self.curr_Omega_tril_list = torch.cholesky(curr_Omega_G)

        for vv in range(self.n_views):

            ## If this view is fixed (template-based alignment), then we don&#39;t need to sample for it.
            if self.fixed_view_idx is not None and (
                vv in self.fixed_view_idx
                if isinstance(self.fixed_view_idx, Iterable)
                else self.fixed_view_idx == vv
            ):
                for mm, mod in enumerate(self.modality_names):
                    observed_X_spatial = X_spatial[mod][view_idx[mod][vv]]
                    G_means[mod][view_idx[mod][vv]] = observed_X_spatial

                    G_samples[mod][:, view_idx[mod][vv], :] = observed_X_spatial

                continue

            kernel_G = lambda x1, x2, diag=False: self.kernel_func_warp(
                x1,
                x2,
                lengthscale_unconstrained=self.warp_kernel_lengthscales[vv],
                output_variance_unconstrained=self.warp_kernel_variances[vv],
                diag=diag,
            )

            ## Collect data from all modalities for this view
            curr_X_spatial_list = []
            curr_n = 0
            curr_mod_idx = []
            for mod in self.modality_names:
                curr_idx = view_idx[mod][vv]
                curr_mod_idx.append(np.arange(curr_n, curr_n + len(curr_idx)))
                curr_n += len(curr_idx)
                curr_modality_and_view_spatial = X_spatial[mod][curr_idx, :]
                curr_X_spatial_list.append(curr_modality_and_view_spatial)

            curr_X_spatial = torch.cat(curr_X_spatial_list, dim=0)

            if len(curr_X_spatial) == 0:
                continue

            curr_X_tilde = self.Xtilde[vv]

            mu_x_G = (
                torch.mm(curr_X_spatial, self.mean_slopes[vv])
                + self.mean_intercepts[vv]
            )

            # Kff_diag = (
            #     kernel_G(curr_X_spatial, curr_X_spatial, diag=True)
            #     + self.diagonal_offset
            # )
            Kff_diag = torch.ones((curr_X_spatial.shape[0]), device=device) * torch.exp(
                self.warp_kernel_variances[vv]
            )

            Kuu = kernel_G(
                curr_X_tilde, curr_X_tilde
            ) + self.diagonal_offset * torch.eye(self.m_X_per_view, device=device)

            Kuf = kernel_G(curr_X_tilde, curr_X_spatial)

            Kuu_chol = torch.cholesky(Kuu)
            self.Kuu_chol_list[vv, :, :] = Kuu_chol

            mu_tilde, Sigma_tilde = self.compute_mean_and_var(
                Kff_diag,
                Kuf,
                Kuu_chol,
                mu_x_G,
                self.mu_z_G,
                self.delta_G_list,
                self.curr_Omega_tril_list,
            )

            # Sample
            G_marginal_dist = torch.distributions.Normal(
                mu_tilde[vv],
                Sigma_tilde[
                    vv * self.n_spatial_dims : vv * self.n_spatial_dims
                    + self.n_spatial_dims
                ].t(),
            )

            for mm, mod in enumerate(self.modality_names):
                curr_idx = curr_mod_idx[mm]
                G_means[mod][view_idx[mod][vv]] = mu_tilde[vv][curr_idx]

            for ss in range(S):

                curr_G_sample = G_marginal_dist.rsample()
                for mm, mod in enumerate(self.modality_names):
                    curr_idx = curr_mod_idx[mm]
                    G_samples[mod][ss, view_idx[mod][vv]] = curr_G_sample[curr_idx]

        self.curr_Omega_tril_F = {}
        for mod in self.modality_names:
            self.curr_Omega_tril_F[mod] = torch.zeros(
                [self.n_latent_outputs[mod], self.m_G, self.m_G], device=device
            )

        F_samples = {}
        self.F_latent_samples = {}
        self.F_observed_samples = {}
        for mod in self.modality_names:
            F_samples[mod] = torch.zeros([S, Ns[mod], self.n_latent_outputs[mod]])
            self.F_latent_samples[mod] = torch.zeros(
                [S, Ns[mod], self.n_latent_outputs[mod]], device=device
            )
            self.F_observed_samples[mod] = torch.zeros([S, Ns[mod], self.Ps[mod]])

        if G_test is not None:

            self.F_latent_samples_test = {}
            self.F_observed_samples_test = {}
            for mod in self.modality_names:
                n_test = G_test[mod].shape[1]
                self.F_latent_samples_test[mod] = torch.zeros(
                    [S, n_test, self.n_latent_outputs[mod]]
                )
                self.F_observed_samples_test[mod] = torch.zeros(
                    [S, n_test, self.Ps[mod]]
                )

        kernel_F = lambda x1, x2, diag=False: self.kernel_func_data(
            x1,
            x2,
            lengthscale_unconstrained=self.data_kernel_lengthscale,
            output_variance_unconstrained=self.data_kernel_variance,
            diag=diag,
        )

        Kuu = kernel_F(self.Gtilde, self.Gtilde) + self.diagonal_offset * torch.eye(
            self.m_G, device=device
        )

        self.Kuu_chol_F = torch.cholesky(Kuu)

        for mod in self.modality_names:

            mu_x_F = torch.zeros([Ns[mod], self.n_latent_outputs[mod]], device=device)
            mu_z_F = torch.zeros([self.m_G, self.n_latent_outputs[mod]], device=device)

            # Kff_diag = (
            #     kernel_F(G_samples[mod], G_samples[mod], diag=True)
            #     + self.diagonal_offset
            # )
            Kff_diag = torch.ones(
                (G_samples[mod].shape[:2]), device=device
            ) * torch.exp(self.data_kernel_variance)

            Kuf = kernel_F(self.Gtilde, G_samples[mod])
            curr_Omega = self.get_Omega_from_Omega_sqt(self.Omega_sqt_F_dict[mod])

            self.curr_Omega_tril_F[mod] = torch.cholesky(curr_Omega)
            mu_tilde, Sigma_tilde = self.compute_mean_and_var(
                Kff_diag,
                Kuf,
                self.Kuu_chol_F,
                mu_x_F,
                mu_z_F,
                self.delta_F_dict[mod],
                self.curr_Omega_tril_F[mod],
            )

            eps = torch.randn(mu_tilde.shape, device=device)
            curr_F_latent_samples = (
                mu_tilde + torch.sqrt(torch.transpose(Sigma_tilde, 1, 2)) * eps
            )

            if self.n_latent_gps[mod] is not None:
                curr_W = self.W_dict[mod]
                F_observed_mean = torch.matmul(curr_F_latent_samples, curr_W)
            else:
                F_observed_mean = curr_F_latent_samples

            self.F_latent_samples[mod] = curr_F_latent_samples
            self.F_observed_samples[mod] = F_observed_mean

            ## For test samples
            if G_test is not None:
                # Kff_diag = (
                #     kernel_F(G_samples[mod], G_samples[mod], diag=True)
                #     + self.diagonal_offset
                # )
                Kff_diag = torch.ones(
                    (G_test[mod].shape[:2]), device=device
                ) * torch.exp(
                    self.data_kernel_variance,
                )

                mu_x_F = torch.zeros(
                    [G_test[mod].shape[1], self.n_latent_outputs[mod]], device=device
                )

                Kuf = kernel_F(self.Gtilde, G_test[mod])

                mu_tilde, Sigma_tilde = self.compute_mean_and_var(
                    Kff_diag,
                    Kuf,
                    self.Kuu_chol_F,
                    mu_x_F,
                    mu_z_F,
                    self.delta_F_dict[mod],
                    self.curr_Omega_tril_F[mod],
                )

                eps = torch.randn(mu_tilde.shape, device=device)
                curr_F_latent_samples = (
                    mu_tilde + torch.sqrt(torch.transpose(Sigma_tilde, 1, 2)) * eps
                )

                if self.n_latent_gps[mod] is not None:
                    curr_W = self.W_dict[mod]
                    F_observed_mean = torch.matmul(curr_F_latent_samples, curr_W)
                else:
                    F_observed_mean = curr_F_latent_samples

                self.F_latent_samples_test[mod] = curr_F_latent_samples
                self.F_observed_samples_test[mod] = F_observed_mean

        if G_test is not None:
            return (
                G_means,
                G_samples,
                self.F_latent_samples,
                self.F_observed_samples,
                self.F_latent_samples_test,
                self.F_observed_samples_test,
            )
        else:
            return G_means, G_samples, self.F_latent_samples, self.F_observed_samples

    def loss_fn(self, data_dict, F_samples):
        # This computes the the negative (approximate) ELBO

        # Running sum for KL terms
        KL_div = 0

        ## G
        for vv in range(self.n_views):
            if self.fixed_view_idx is not None and (
                vv in self.fixed_view_idx
                if isinstance(self.fixed_view_idx, Iterable)
                else self.fixed_view_idx == vv
            ):
                continue
            for jj in range(self.n_spatial_dims):
                qu = torch.distributions.MultivariateNormal(
                    loc=self.delta_G_list[vv, :, jj],
                    scale_tril=self.curr_Omega_tril_list[jj * self.n_views + vv, :, :],
                )
                pu = torch.distributions.MultivariateNormal(
                    loc=self.mu_z_G[vv, :, jj],
                    scale_tril=self.Kuu_chol_list[vv, :, :],
                )
                curr_KL_div = torch.distributions.kl.kl_divergence(qu, pu)

                KL_div += curr_KL_div

        ## F
        LL = 0
        pu = torch.distributions.MultivariateNormal(
            loc=torch.zeros(self.m_G, device=device), scale_tril=self.Kuu_chol_F
        )
        for mm, mod in enumerate(self.modality_names):
            qu = torch.distributions.MultivariateNormal(
                loc=self.delta_F_dict[mod].t(),
                scale_tril=self.curr_Omega_tril_F[mod],
            )

            curr_KL_div = torch.distributions.kl.kl_divergence(qu, pu)
            KL_div += curr_KL_div.sum()

            Y_distribution = torch.distributions.Normal(
                loc=F_samples[mod],
                scale=self.noise_variance_pos[-self.n_modalities + mm],
            )
            S = F_samples[mod].shape[0]

            LL += Y_distribution.log_prob(data_dict[mod][&#34;outputs&#34;]).sum() / S

        return -LL + KL_div


if __name__ == &#34;__main__&#34;:
    pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gpsa.models.vgpsa.VariationalGPSA"><code class="flex name class">
<span>class <span class="ident">VariationalGPSA</span></span>
<span>(</span><span>data_dict, m_X_per_view, m_G, data_init=True, minmax_init=False, grid_init=False, n_spatial_dims=2, n_noise_variance_params=2, kernel_func_warp=&lt;function rbf_kernel&gt;, kernel_func_data=&lt;function rbf_kernel&gt;, n_latent_gps=None, mean_function='identity_fixed', mean_penalty_param=0.0, fixed_warp_kernel_variances=None, fixed_warp_kernel_lengthscales=None, fixed_data_kernel_lengthscales=None, fixed_view_idx=None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>data_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of data in the format {"modality": {"spatial_coords": X, "outputs": Y, "n_samples_list": n_samples_list}}</dd>
<dt><strong><code>data_init</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to initialize inducing locations with KMeans on data.</dd>
<dt><strong><code>n_spatial_dims</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of spatial dimensions (usually 2 or 3).</dd>
<dt><strong><code>n_noise_variance_params</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of noise variance parameters.</dd>
<dt><strong><code>kernel_func_warp</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>Covariance function for warp GP.</dd>
<dt><strong><code>kernel_func_data</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>Covariance function for output GP.</dd>
<dt><strong><code>mean_function</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Mean function for warp GP. One of ["identity_fixed", "identity_initialized", or None]. None results in a linear mean function.</dd>
<dt><strong><code>mean_penalty_param</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Description</dd>
<dt><strong><code>fixed_warp_kernel_variances</code></strong> :&ensp;<code>None</code>, optional</dt>
<dd>Description</dd>
<dt><strong><code>fixed_warp_kernel_lengthscales</code></strong> :&ensp;<code>None</code>, optional</dt>
<dd>Description</dd>
<dt><strong><code>fixed_data_kernel_lengthscales</code></strong> :&ensp;<code>None</code>, optional</dt>
<dd>Description</dd>
</dl>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VariationalGPSA(GPSA):
    def __init__(
        self,
        data_dict,
        m_X_per_view,
        m_G,
        data_init=True,
        minmax_init=False,
        grid_init=False,
        n_spatial_dims=2,
        n_noise_variance_params=2,
        kernel_func_warp=rbf_kernel,
        kernel_func_data=rbf_kernel,
        n_latent_gps=None,
        mean_function=&#34;identity_fixed&#34;,
        mean_penalty_param=0.0,
        fixed_warp_kernel_variances=None,
        fixed_warp_kernel_lengthscales=None,
        fixed_data_kernel_lengthscales=None,
        fixed_view_idx=None,
    ):
        super(VariationalGPSA, self).__init__(
            data_dict,
            data_init=True,
            n_spatial_dims=2,
            n_noise_variance_params=2,
            kernel_func_warp=kernel_func_warp,
            kernel_func_data=kernel_func_data,
            mean_penalty_param=mean_penalty_param,
            fixed_warp_kernel_variances=fixed_warp_kernel_variances,
            fixed_warp_kernel_lengthscales=fixed_warp_kernel_lengthscales,
            fixed_data_kernel_lengthscales=fixed_data_kernel_lengthscales,
        )

        self.m_X_per_view = m_X_per_view
        self.m_G = m_G
        self.n_latent_gps = n_latent_gps
        self.n_latent_outputs = {}
        for mod in self.modality_names:
            curr_n_latent_outputs = (
                self.n_latent_gps[mod]
                if self.n_latent_gps[mod] is not None
                else self.Ps[mod]
            )
            self.n_latent_outputs[mod] = curr_n_latent_outputs
        self.fixed_view_idx = fixed_view_idx

        if data_init:
            # Initialize inducing locations with a subset of the data
            Xtilde = torch.zeros([self.n_views, self.m_X_per_view, self.n_spatial_dims])
            for ii in range(self.n_views):
                curr_X_spatial_list = []
                for mod in self.modality_names:
                    curr_idx = self.view_idx[mod][ii]
                    curr_modality_and_view_spatial = data_dict[mod][&#34;spatial_coords&#34;][
                        curr_idx, :
                    ]
                    curr_X_spatial_list.append(curr_modality_and_view_spatial)
                curr_X_spatial = torch.cat(curr_X_spatial_list, dim=0)

                kmeans = KMeans(n_clusters=self.m_X_per_view)
                kmeans.fit(curr_X_spatial.detach().cpu().numpy())
                Xtilde[ii, :, :] = torch.tensor(kmeans.cluster_centers_)

            self.Xtilde = nn.Parameter(Xtilde.clone())
            # self.Xtilde = Xtilde.clone()

            rand_idx = np.random.choice(
                np.arange(curr_X_spatial.shape[0]),
                size=self.m_G,
                replace=False,
            )

            all_X_spatial = torch.cat(
                [data_dict[mod][&#34;spatial_coords&#34;] for mod in self.modality_names]
            )
            kmeans = KMeans(n_clusters=self.m_G)
            kmeans.fit(all_X_spatial.detach().cpu().numpy())
            self.Gtilde = nn.Parameter(torch.tensor(kmeans.cluster_centers_))

        elif grid_init:

            if self.n_spatial_dims == 2:
                xlow, ylow = (
                    data_dict[self.modality_names[0]][&#34;spatial_coords&#34;].numpy().min(0)
                )
                xhigh, yhigh = (
                    data_dict[self.modality_names[0]][&#34;spatial_coords&#34;].numpy().max(0)
                )
                xlimits = [xlow, xhigh]
                ylimits = [ylow, yhigh]
                numticks = np.ceil(np.sqrt(self.m_G)).astype(int)
                self.m_G = numticks**2
                self.m_X_per_view = numticks**2
                x1s = np.linspace(*xlimits, num=numticks)
                x2s = np.linspace(*ylimits, num=numticks)
                X1, X2 = np.meshgrid(x1s, x2s)
                Xtilde = np.vstack([X1.ravel(), X2.ravel()]).T
                Xtilde_torch = torch.zeros(
                    [self.n_views, Xtilde.shape[0], self.n_spatial_dims]
                )
                for vv in range(self.n_views):
                    Xtilde_torch[vv] = torch.tensor(Xtilde)

                # self.Xtilde = Xtilde_torch.clone()
                # self.Gtilde = torch.tensor(Xtilde).float()
                self.Xtilde = nn.Parameter(Xtilde_torch.clone())
                self.Gtilde = nn.Parameter(torch.tensor(Xtilde).float())

        else:
            # Random initialization of inducing locations
            self.Xtilde = nn.Parameter(
                torch.randn([self.n_views, self.m_X_per_view, self.n_spatial_dims])
            )
            self.Gtilde = nn.Parameter(torch.randn([self.m_G, self.n_spatial_dims]))

        ## Variational covariance parameters
        Omega_sqt_G_list = torch.zeros(
            [self.n_views * self.n_spatial_dims, self.m_X_per_view, self.m_X_per_view],
            device=device,
        )
        for ii in range(self.n_views):
            for jj in range(self.n_spatial_dims):
                Omega_sqt = 0.1 * torch.randn(
                    size=[self.m_X_per_view, self.m_X_per_view]
                )
                # import ipdb; ipdb.set_trace()
                # Omega_sqt_G_list[ii * self.n_views + jj, :, :] = Omega_sqt
                Omega_sqt_G_list[jj * self.n_views + ii, :, :] = Omega_sqt
        self.Omega_sqt_G_list = nn.Parameter(Omega_sqt_G_list)

        Omega_sqt_F_dict = torch.nn.ParameterDict()
        for mod in self.modality_names:
            num_outputs = self.Ps[mod]
            curr_Omega = torch.zeros([self.n_latent_outputs[mod], self.m_G, self.m_G])
            for jj in range(self.n_latent_outputs[mod]):
                Omega_sqt = 0.1 * torch.randn(size=[self.m_G, self.m_G])
                curr_Omega[jj, :, :] = Omega_sqt
            Omega_sqt_F_dict[mod] = nn.Parameter(curr_Omega)
        self.Omega_sqt_F_dict = Omega_sqt_F_dict

        ## Variational mean parameters
        self.delta_G_list = nn.Parameter(self.Xtilde.clone())
        delta_F_dict = torch.nn.ParameterDict()
        for mod in self.modality_names:
            num_outputs = self.Ps[mod]
            curr_delta = nn.Parameter(
                torch.randn(size=[self.m_G, self.n_latent_outputs[mod]], device=device)
            )
            delta_F_dict[mod] = curr_delta
        self.delta_F_dict = delta_F_dict

        ## LMC parameters
        self.W_dict = torch.nn.ParameterDict()
        for mod in self.modality_names:
            if self.n_latent_gps[mod] is not None:
                self.W_dict[mod] = nn.Parameter(
                    torch.randn([self.n_latent_gps[mod], self.Ps[mod]])
                )

    def compute_mean_and_var(
        self, Kff_diag, Kuf, Kuu_chol, mu_x, mu_z, delta, Omega_tril
    ):
        alpha_x = torch.cholesky_solve(Kuf, Kuu_chol)

        a_t_Kchol = torch.matmul(alpha_x.transpose(-1, -2), Kuu_chol)
        aKa = torch.sum(torch.square(a_t_Kchol), dim=-1)

        mu_tilde = mu_x.unsqueeze(0) + torch.matmul(
            alpha_x.transpose(-1, -2), delta - mu_z
        )

        if len(alpha_x.shape) == 2:
            a_t_Omega_tril = torch.matmul(
                alpha_x.transpose(-1, -2).unsqueeze(0), Omega_tril
            )
            aOmega_a = torch.sum(torch.square(a_t_Omega_tril), dim=-1)
            Sigma_tilde = Kff_diag - aKa + aOmega_a + self.diagonal_offset
        else:
            a_t_Omega_tril = torch.matmul(
                alpha_x.transpose(-1, -2).unsqueeze(1), Omega_tril.unsqueeze(0)
            )
            aOmega_a = torch.sum(torch.square(a_t_Omega_tril), dim=-1)
            Sigma_tilde = (
                Kff_diag.unsqueeze(1)
                - aKa.unsqueeze(1)
                + aOmega_a
                + self.diagonal_offset
            )

        return mu_tilde, Sigma_tilde + self.diagonal_offset

    def get_Omega_from_Omega_sqt(self, Omega_sqt):
        return torch.matmul(
            Omega_sqt,
            torch.transpose(Omega_sqt, -1, -2),
        ) + self.diagonal_offset * torch.eye(Omega_sqt.shape[-1], device=device)

    def forward(self, X_spatial, view_idx, Ns, S=1, prediction_mode=False, G_test=None):

        if prediction_mode:
            self.eval()

        self.noise_variance_pos = torch.exp(self.noise_variance) + self.diagonal_offset

        self.mu_z_G = (
            torch.zeros(
                [self.n_views, self.m_X_per_view, self.n_spatial_dims], device=device
            )
            * np.nan
        )
        for vv in range(self.n_views):
            self.mu_z_G[vv] = (
                torch.mm(self.Xtilde[vv], self.mean_slopes[vv])
                + self.mean_intercepts[vv]
            )
            if self.fixed_view_idx is not None and (
                vv in self.fixed_view_idx
                if isinstance(self.fixed_view_idx, Iterable)
                else self.fixed_view_idx == vv
            ):
                self.mu_z_G[vv] *= 100.0

        self.Kuu_chol_list = (
            torch.zeros(
                [self.n_views, self.m_X_per_view, self.m_X_per_view], device=device
            )
            * np.nan
        )
        G_samples = {}
        for mod in self.modality_names:
            G_samples[mod] = (
                torch.zeros([S, Ns[mod], self.n_spatial_dims], device=device) * np.nan
            )

        G_means = {}
        for mod in self.modality_names:
            G_means[mod] = (
                torch.zeros([Ns[mod], self.n_spatial_dims], device=device) * np.nan
            )

        curr_Omega_G = self.get_Omega_from_Omega_sqt(self.Omega_sqt_G_list)

        self.curr_Omega_tril_list = torch.cholesky(curr_Omega_G)

        for vv in range(self.n_views):

            ## If this view is fixed (template-based alignment), then we don&#39;t need to sample for it.
            if self.fixed_view_idx is not None and (
                vv in self.fixed_view_idx
                if isinstance(self.fixed_view_idx, Iterable)
                else self.fixed_view_idx == vv
            ):
                for mm, mod in enumerate(self.modality_names):
                    observed_X_spatial = X_spatial[mod][view_idx[mod][vv]]
                    G_means[mod][view_idx[mod][vv]] = observed_X_spatial

                    G_samples[mod][:, view_idx[mod][vv], :] = observed_X_spatial

                continue

            kernel_G = lambda x1, x2, diag=False: self.kernel_func_warp(
                x1,
                x2,
                lengthscale_unconstrained=self.warp_kernel_lengthscales[vv],
                output_variance_unconstrained=self.warp_kernel_variances[vv],
                diag=diag,
            )

            ## Collect data from all modalities for this view
            curr_X_spatial_list = []
            curr_n = 0
            curr_mod_idx = []
            for mod in self.modality_names:
                curr_idx = view_idx[mod][vv]
                curr_mod_idx.append(np.arange(curr_n, curr_n + len(curr_idx)))
                curr_n += len(curr_idx)
                curr_modality_and_view_spatial = X_spatial[mod][curr_idx, :]
                curr_X_spatial_list.append(curr_modality_and_view_spatial)

            curr_X_spatial = torch.cat(curr_X_spatial_list, dim=0)

            if len(curr_X_spatial) == 0:
                continue

            curr_X_tilde = self.Xtilde[vv]

            mu_x_G = (
                torch.mm(curr_X_spatial, self.mean_slopes[vv])
                + self.mean_intercepts[vv]
            )

            # Kff_diag = (
            #     kernel_G(curr_X_spatial, curr_X_spatial, diag=True)
            #     + self.diagonal_offset
            # )
            Kff_diag = torch.ones((curr_X_spatial.shape[0]), device=device) * torch.exp(
                self.warp_kernel_variances[vv]
            )

            Kuu = kernel_G(
                curr_X_tilde, curr_X_tilde
            ) + self.diagonal_offset * torch.eye(self.m_X_per_view, device=device)

            Kuf = kernel_G(curr_X_tilde, curr_X_spatial)

            Kuu_chol = torch.cholesky(Kuu)
            self.Kuu_chol_list[vv, :, :] = Kuu_chol

            mu_tilde, Sigma_tilde = self.compute_mean_and_var(
                Kff_diag,
                Kuf,
                Kuu_chol,
                mu_x_G,
                self.mu_z_G,
                self.delta_G_list,
                self.curr_Omega_tril_list,
            )

            # Sample
            G_marginal_dist = torch.distributions.Normal(
                mu_tilde[vv],
                Sigma_tilde[
                    vv * self.n_spatial_dims : vv * self.n_spatial_dims
                    + self.n_spatial_dims
                ].t(),
            )

            for mm, mod in enumerate(self.modality_names):
                curr_idx = curr_mod_idx[mm]
                G_means[mod][view_idx[mod][vv]] = mu_tilde[vv][curr_idx]

            for ss in range(S):

                curr_G_sample = G_marginal_dist.rsample()
                for mm, mod in enumerate(self.modality_names):
                    curr_idx = curr_mod_idx[mm]
                    G_samples[mod][ss, view_idx[mod][vv]] = curr_G_sample[curr_idx]

        self.curr_Omega_tril_F = {}
        for mod in self.modality_names:
            self.curr_Omega_tril_F[mod] = torch.zeros(
                [self.n_latent_outputs[mod], self.m_G, self.m_G], device=device
            )

        F_samples = {}
        self.F_latent_samples = {}
        self.F_observed_samples = {}
        for mod in self.modality_names:
            F_samples[mod] = torch.zeros([S, Ns[mod], self.n_latent_outputs[mod]])
            self.F_latent_samples[mod] = torch.zeros(
                [S, Ns[mod], self.n_latent_outputs[mod]], device=device
            )
            self.F_observed_samples[mod] = torch.zeros([S, Ns[mod], self.Ps[mod]])

        if G_test is not None:

            self.F_latent_samples_test = {}
            self.F_observed_samples_test = {}
            for mod in self.modality_names:
                n_test = G_test[mod].shape[1]
                self.F_latent_samples_test[mod] = torch.zeros(
                    [S, n_test, self.n_latent_outputs[mod]]
                )
                self.F_observed_samples_test[mod] = torch.zeros(
                    [S, n_test, self.Ps[mod]]
                )

        kernel_F = lambda x1, x2, diag=False: self.kernel_func_data(
            x1,
            x2,
            lengthscale_unconstrained=self.data_kernel_lengthscale,
            output_variance_unconstrained=self.data_kernel_variance,
            diag=diag,
        )

        Kuu = kernel_F(self.Gtilde, self.Gtilde) + self.diagonal_offset * torch.eye(
            self.m_G, device=device
        )

        self.Kuu_chol_F = torch.cholesky(Kuu)

        for mod in self.modality_names:

            mu_x_F = torch.zeros([Ns[mod], self.n_latent_outputs[mod]], device=device)
            mu_z_F = torch.zeros([self.m_G, self.n_latent_outputs[mod]], device=device)

            # Kff_diag = (
            #     kernel_F(G_samples[mod], G_samples[mod], diag=True)
            #     + self.diagonal_offset
            # )
            Kff_diag = torch.ones(
                (G_samples[mod].shape[:2]), device=device
            ) * torch.exp(self.data_kernel_variance)

            Kuf = kernel_F(self.Gtilde, G_samples[mod])
            curr_Omega = self.get_Omega_from_Omega_sqt(self.Omega_sqt_F_dict[mod])

            self.curr_Omega_tril_F[mod] = torch.cholesky(curr_Omega)
            mu_tilde, Sigma_tilde = self.compute_mean_and_var(
                Kff_diag,
                Kuf,
                self.Kuu_chol_F,
                mu_x_F,
                mu_z_F,
                self.delta_F_dict[mod],
                self.curr_Omega_tril_F[mod],
            )

            eps = torch.randn(mu_tilde.shape, device=device)
            curr_F_latent_samples = (
                mu_tilde + torch.sqrt(torch.transpose(Sigma_tilde, 1, 2)) * eps
            )

            if self.n_latent_gps[mod] is not None:
                curr_W = self.W_dict[mod]
                F_observed_mean = torch.matmul(curr_F_latent_samples, curr_W)
            else:
                F_observed_mean = curr_F_latent_samples

            self.F_latent_samples[mod] = curr_F_latent_samples
            self.F_observed_samples[mod] = F_observed_mean

            ## For test samples
            if G_test is not None:
                # Kff_diag = (
                #     kernel_F(G_samples[mod], G_samples[mod], diag=True)
                #     + self.diagonal_offset
                # )
                Kff_diag = torch.ones(
                    (G_test[mod].shape[:2]), device=device
                ) * torch.exp(
                    self.data_kernel_variance,
                )

                mu_x_F = torch.zeros(
                    [G_test[mod].shape[1], self.n_latent_outputs[mod]], device=device
                )

                Kuf = kernel_F(self.Gtilde, G_test[mod])

                mu_tilde, Sigma_tilde = self.compute_mean_and_var(
                    Kff_diag,
                    Kuf,
                    self.Kuu_chol_F,
                    mu_x_F,
                    mu_z_F,
                    self.delta_F_dict[mod],
                    self.curr_Omega_tril_F[mod],
                )

                eps = torch.randn(mu_tilde.shape, device=device)
                curr_F_latent_samples = (
                    mu_tilde + torch.sqrt(torch.transpose(Sigma_tilde, 1, 2)) * eps
                )

                if self.n_latent_gps[mod] is not None:
                    curr_W = self.W_dict[mod]
                    F_observed_mean = torch.matmul(curr_F_latent_samples, curr_W)
                else:
                    F_observed_mean = curr_F_latent_samples

                self.F_latent_samples_test[mod] = curr_F_latent_samples
                self.F_observed_samples_test[mod] = F_observed_mean

        if G_test is not None:
            return (
                G_means,
                G_samples,
                self.F_latent_samples,
                self.F_observed_samples,
                self.F_latent_samples_test,
                self.F_observed_samples_test,
            )
        else:
            return G_means, G_samples, self.F_latent_samples, self.F_observed_samples

    def loss_fn(self, data_dict, F_samples):
        # This computes the the negative (approximate) ELBO

        # Running sum for KL terms
        KL_div = 0

        ## G
        for vv in range(self.n_views):
            if self.fixed_view_idx is not None and (
                vv in self.fixed_view_idx
                if isinstance(self.fixed_view_idx, Iterable)
                else self.fixed_view_idx == vv
            ):
                continue
            for jj in range(self.n_spatial_dims):
                qu = torch.distributions.MultivariateNormal(
                    loc=self.delta_G_list[vv, :, jj],
                    scale_tril=self.curr_Omega_tril_list[jj * self.n_views + vv, :, :],
                )
                pu = torch.distributions.MultivariateNormal(
                    loc=self.mu_z_G[vv, :, jj],
                    scale_tril=self.Kuu_chol_list[vv, :, :],
                )
                curr_KL_div = torch.distributions.kl.kl_divergence(qu, pu)

                KL_div += curr_KL_div

        ## F
        LL = 0
        pu = torch.distributions.MultivariateNormal(
            loc=torch.zeros(self.m_G, device=device), scale_tril=self.Kuu_chol_F
        )
        for mm, mod in enumerate(self.modality_names):
            qu = torch.distributions.MultivariateNormal(
                loc=self.delta_F_dict[mod].t(),
                scale_tril=self.curr_Omega_tril_F[mod],
            )

            curr_KL_div = torch.distributions.kl.kl_divergence(qu, pu)
            KL_div += curr_KL_div.sum()

            Y_distribution = torch.distributions.Normal(
                loc=F_samples[mod],
                scale=self.noise_variance_pos[-self.n_modalities + mm],
            )
            S = F_samples[mod].shape[0]

            LL += Y_distribution.log_prob(data_dict[mod][&#34;outputs&#34;]).sum() / S

        return -LL + KL_div</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="gpsa.models.gpsa.GPSA" href="gpsa.html#gpsa.models.gpsa.GPSA">GPSA</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="gpsa.models.vgpsa.VariationalGPSA.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gpsa.models.vgpsa.VariationalGPSA.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="gpsa.models.vgpsa.VariationalGPSA.compute_mean_and_var"><code class="name flex">
<span>def <span class="ident">compute_mean_and_var</span></span>(<span>self, Kff_diag, Kuf, Kuu_chol, mu_x, mu_z, delta, Omega_tril)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_mean_and_var(
    self, Kff_diag, Kuf, Kuu_chol, mu_x, mu_z, delta, Omega_tril
):
    alpha_x = torch.cholesky_solve(Kuf, Kuu_chol)

    a_t_Kchol = torch.matmul(alpha_x.transpose(-1, -2), Kuu_chol)
    aKa = torch.sum(torch.square(a_t_Kchol), dim=-1)

    mu_tilde = mu_x.unsqueeze(0) + torch.matmul(
        alpha_x.transpose(-1, -2), delta - mu_z
    )

    if len(alpha_x.shape) == 2:
        a_t_Omega_tril = torch.matmul(
            alpha_x.transpose(-1, -2).unsqueeze(0), Omega_tril
        )
        aOmega_a = torch.sum(torch.square(a_t_Omega_tril), dim=-1)
        Sigma_tilde = Kff_diag - aKa + aOmega_a + self.diagonal_offset
    else:
        a_t_Omega_tril = torch.matmul(
            alpha_x.transpose(-1, -2).unsqueeze(1), Omega_tril.unsqueeze(0)
        )
        aOmega_a = torch.sum(torch.square(a_t_Omega_tril), dim=-1)
        Sigma_tilde = (
            Kff_diag.unsqueeze(1)
            - aKa.unsqueeze(1)
            + aOmega_a
            + self.diagonal_offset
        )

    return mu_tilde, Sigma_tilde + self.diagonal_offset</code></pre>
</details>
</dd>
<dt id="gpsa.models.vgpsa.VariationalGPSA.get_Omega_from_Omega_sqt"><code class="name flex">
<span>def <span class="ident">get_Omega_from_Omega_sqt</span></span>(<span>self, Omega_sqt)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_Omega_from_Omega_sqt(self, Omega_sqt):
    return torch.matmul(
        Omega_sqt,
        torch.transpose(Omega_sqt, -1, -2),
    ) + self.diagonal_offset * torch.eye(Omega_sqt.shape[-1], device=device)</code></pre>
</details>
</dd>
<dt id="gpsa.models.vgpsa.VariationalGPSA.loss_fn"><code class="name flex">
<span>def <span class="ident">loss_fn</span></span>(<span>self, data_dict, F_samples)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loss_fn(self, data_dict, F_samples):
    # This computes the the negative (approximate) ELBO

    # Running sum for KL terms
    KL_div = 0

    ## G
    for vv in range(self.n_views):
        if self.fixed_view_idx is not None and (
            vv in self.fixed_view_idx
            if isinstance(self.fixed_view_idx, Iterable)
            else self.fixed_view_idx == vv
        ):
            continue
        for jj in range(self.n_spatial_dims):
            qu = torch.distributions.MultivariateNormal(
                loc=self.delta_G_list[vv, :, jj],
                scale_tril=self.curr_Omega_tril_list[jj * self.n_views + vv, :, :],
            )
            pu = torch.distributions.MultivariateNormal(
                loc=self.mu_z_G[vv, :, jj],
                scale_tril=self.Kuu_chol_list[vv, :, :],
            )
            curr_KL_div = torch.distributions.kl.kl_divergence(qu, pu)

            KL_div += curr_KL_div

    ## F
    LL = 0
    pu = torch.distributions.MultivariateNormal(
        loc=torch.zeros(self.m_G, device=device), scale_tril=self.Kuu_chol_F
    )
    for mm, mod in enumerate(self.modality_names):
        qu = torch.distributions.MultivariateNormal(
            loc=self.delta_F_dict[mod].t(),
            scale_tril=self.curr_Omega_tril_F[mod],
        )

        curr_KL_div = torch.distributions.kl.kl_divergence(qu, pu)
        KL_div += curr_KL_div.sum()

        Y_distribution = torch.distributions.Normal(
            loc=F_samples[mod],
            scale=self.noise_variance_pos[-self.n_modalities + mm],
        )
        S = F_samples[mod].shape[0]

        LL += Y_distribution.log_prob(data_dict[mod][&#34;outputs&#34;]).sum() / S

    return -LL + KL_div</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="gpsa.models.gpsa.GPSA" href="gpsa.html#gpsa.models.gpsa.GPSA">GPSA</a></b></code>:
<ul class="hlist">
<li><code><a title="gpsa.models.gpsa.GPSA.create_view_idx_dict" href="gpsa.html#gpsa.models.gpsa.GPSA.create_view_idx_dict">create_view_idx_dict</a></code></li>
<li><code><a title="gpsa.models.gpsa.GPSA.forward" href="gpsa.html#gpsa.models.gpsa.GPSA.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gpsa.models" href="index.html">gpsa.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gpsa.models.vgpsa.VariationalGPSA" href="#gpsa.models.vgpsa.VariationalGPSA">VariationalGPSA</a></code></h4>
<ul class="">
<li><code><a title="gpsa.models.vgpsa.VariationalGPSA.compute_mean_and_var" href="#gpsa.models.vgpsa.VariationalGPSA.compute_mean_and_var">compute_mean_and_var</a></code></li>
<li><code><a title="gpsa.models.vgpsa.VariationalGPSA.dump_patches" href="#gpsa.models.vgpsa.VariationalGPSA.dump_patches">dump_patches</a></code></li>
<li><code><a title="gpsa.models.vgpsa.VariationalGPSA.get_Omega_from_Omega_sqt" href="#gpsa.models.vgpsa.VariationalGPSA.get_Omega_from_Omega_sqt">get_Omega_from_Omega_sqt</a></code></li>
<li><code><a title="gpsa.models.vgpsa.VariationalGPSA.loss_fn" href="#gpsa.models.vgpsa.VariationalGPSA.loss_fn">loss_fn</a></code></li>
<li><code><a title="gpsa.models.vgpsa.VariationalGPSA.training" href="#gpsa.models.vgpsa.VariationalGPSA.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>